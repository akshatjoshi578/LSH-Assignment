import json
import re
import random
from collections import defaultdict, Counter
import numpy as np


DATASET_FILE = "/Users/miteshjoshi/Downloads/TVs-all-merged 2.json"

NUM_BOOTSTRAPS = 20

Q_GRAM = 3

coarse_low  = np.arange(0.10, 0.61, 0.05)
fine_high   = np.arange(0.60, 0.86, 0.01)

T_VALUES = sorted(
    set(round(float(x), 3) for x in np.concatenate([coarse_low, fine_high]))
)

#MSM grid (inspired by paper)
ALPHA_GRID = [0.5, 0.6, 0.7]
BETA_GRID = [0.0]
GAMMA_GRID = [0.75]
MU_GRID = [0.5, 0.6, 0.7]
EPS_GRID = [0.4, 0.5, 0.6, 0.7]

#new step
TITLE_STOPWORDS = set()


def q_grams(s, q=Q_GRAM):
    s = " " + s + " "
    if len(s) < q:
        return {s}
    return {s[i:i + q] for i in range(len(s) - q + 1)}


def q_gram_sim(s1, s2, q=Q_GRAM):
    if not s1 or not s2:
        return 0.0
    g1 = q_grams(s1, q)
    g2 = q_grams(s2, q)
    if not g1 and not g2:
        return 0.0
    inter = len(g1 & g2)
    uni = len(g1 | g2)
    return inter / uni if uni > 0 else 0.0


def choose_b_r(n, target_t):
    best_b = 1
    best_r_eff = float(n)
    best_t_est = (1.0 / best_b) ** (1.0 / best_r_eff)
    best_diff = abs(best_t_est - target_t)

    for b in range(1, n + 1):
        r_eff = n / b
        if r_eff <= 0:
            continue
        t_est = (1.0 / b) ** (1.0 / r_eff)
        diff = abs(t_est - target_t)
        if diff < best_diff:
            best_diff = diff
            best_b = b
            best_r_eff = r_eff
            best_t_est = t_est

    return best_b, best_r_eff, best_t_est

#data cleaner class
class DataCleaner:
    @staticmethod
    def clean_value(text):
        if not isinstance(text, str):
            return ""
        text = text.lower()
        #normalize inch
        text = re.sub(r'(\s|-)?(inch(es)?|in\.|")', "inch", text)
        #normalize hz
        text = re.sub(r'(\s|-)?(hertz|hz)', "hz", text)
        #remove non-alphanumeric except dot & space
        text = re.sub(r"[^\w\s\.]", "", text)
        return text.strip()

    @staticmethod
    def extract_title_model_words(title):
        if not isinstance(title, str):
            return set()
        clean = DataCleaner.clean_value(title)
        tokens = clean.split()
        model_words = set()
        regex = re.compile(
            r"^[a-zA-Z0-9]*"
            r"(([0-9]+[^0-9, ]+)|([^0-9, ]+[0-9]+))"
            r"[a-zA-Z0-9]*$"
        )
        for tok in tokens:
            if tok in TITLE_STOPWORDS:
                continue
            if regex.match(tok):
                model_words.add(tok)
        return model_words

    @staticmethod
    def extract_kvp_model_words(attributes):
        model_words = set()
        for v in attributes.values():
            if not isinstance(v, str):
                continue
            clean_v = DataCleaner.clean_value(v)
            for tok in clean_v.split():
                if re.match(r"^\d+(\.\d+)?[a-zA-Z]*$", tok):
                    m = re.match(r"^(\d+(?:\.\d+)?)", tok)
                    if m:
                        model_words.add(m.group(1))
        return model_words

    @staticmethod
    def extract_brand(title, attributes):
        for key in ["brand", "Brand", "BRAND", "manufacturer", "Manufacturer"]:
            val = attributes.get(key)
            if isinstance(val, str) and val.strip():
                return val.strip().lower()
        if isinstance(title, str) and title.strip():
            return title.strip().split()[0].lower()
        return None


class Product:
    def __init__(self, shop, model_id, title, attributes, original_idx):
        self.shop = shop
        self.model_id = model_id
        self.title = title or ""
        self.attributes = attributes or {}
        self.original_idx = original_idx

        self.clean_title = DataCleaner.clean_value(self.title)
        self.mw_title = DataCleaner.extract_title_model_words(self.clean_title)
        self.mw_kvp = DataCleaner.extract_kvp_model_words(self.attributes)
        self.model_words = self.mw_title | self.mw_kvp
        self.brand = DataCleaner.extract_brand(self.title, self.attributes)


class Instance:
    def __init__(self, inst_id, product: Product):
        self.id = inst_id
        self.product = product
        self.shop = product.shop
        self.model_id = product.model_id
        self.attributes = product.attributes
        self.mw_title = product.mw_title
        self.model_words = product.model_words
        self.clean_title = product.clean_title
        self.brand = product.brand
        self.signature = None


def compute_title_stopwords(raw_items, threshold=0.30):
    df = Counter()
    n_docs = 0

    for item in raw_items:
        title = item.get("title", "") or ""
        clean = DataCleaner.clean_value(title)
        toks = set(clean.split())
        if not toks:
            continue
        n_docs += 1
        for t in toks:
            df[t] += 1

    if n_docs == 0:
        return set()

    stopwords = {t for t, c in df.items() if c / n_docs > threshold}
    return stopwords

#lsh and minhasing
class LSH:
    def __init__(self, n, vocab_size):
        self.n = n
        self.vocab_size = vocab_size
        self.p = 4294967311  # large prime
        self.hash_coeffs = self._generate_hash_coeffs()

    def _generate_hash_coeffs(self):
        coeffs = []
        for _ in range(self.n):
            a = random.randint(1, self.p - 1)
            b = random.randint(0, self.p - 1)
            coeffs.append((a, b))
        return coeffs

    def compute_signatures(self, instances, vocab_map):
        for inst in instances:
            idxs = [vocab_map[w] for w in inst.model_words if w in vocab_map]
            if not idxs:
                inst.signature = [float("inf")] * self.n
                continue
            sig = []
            for a, b in self.hash_coeffs:
                m_hash = float("inf")
                for idx in idxs:
                    h = (a * idx + b) % self.p
                    if h < m_hash:
                        m_hash = h
                sig.append(m_hash)
            inst.signature = sig

    def get_candidates(self, instances, b):

        n = self.n
        rows_per_band = n // b
        remainder = n % b

        candidates = set()
        start = 0
        for band_idx in range(b):
            r = rows_per_band + (remainder if band_idx == b - 1 else 0)
            end = start + r
            buckets = defaultdict(list)
            for inst in instances:
                band_sig = tuple(inst.signature[start:end])
                buckets[band_sig].append(inst)
            for bucket_insts in buckets.values():
                if len(bucket_insts) > 1:
                    bucket_insts = sorted(bucket_insts, key=lambda x: x.id)
                    for i in range(len(bucket_insts)):
                        for j in range(i + 1, len(bucket_insts)):
                            a = bucket_insts[i]
                            c = bucket_insts[j]
                            # Shop & brand filter (MSM's diffBrand / sameShop rules)
                            if a.shop == c.shop:
                                continue
                            if a.brand and c.brand and a.brand != c.brand:
                                continue
                            pair = (a.id, c.id)
                            candidates.add(pair)
            start = end
        return candidates


#msm clustering
def tmwm_title_sim(p1: Product, p2: Product, alpha: float, beta: float):
    name_sim = q_gram_sim(p1.clean_title, p2.clean_title)
    if name_sim >= alpha:
        return 1.0

    mw1 = p1.mw_title
    mw2 = p2.mw_title
    if mw1 or mw2:
        inter = len(mw1 & mw2)
        uni = len(mw1 | mw2)
        mw_sim = inter / uni if uni > 0 else 0.0
    else:
        mw_sim = 0.0

    if mw_sim >= beta:
        return mw_sim
    return -1.0


def compute_structural_components(p1: Product, p2: Product, gamma: float):

    keys1 = list(p1.attributes.keys())
    keys2 = list(p2.attributes.keys())
    nmki = set(keys1)
    nmkj = set(keys2)

    sim_sum = 0.0
    weight_sum = 0.0
    m = 0

    for k1 in keys1:
        best_sim = 0.0
        best_k2 = None
        for k2 in keys2:
            if k2 not in nmkj:
                continue
            keySim = q_gram_sim(k1.lower(), k2.lower())
            if keySim > best_sim:
                best_sim = keySim
                best_k2 = k2
        if best_k2 is not None and best_sim > gamma:
            nmki.discard(k1)
            nmkj.discard(best_k2)
            v1 = str(p1.attributes.get(k1, ""))
            v2 = str(p2.attributes.get(best_k2, ""))
            valueSim = q_gram_sim(v1.lower(), v2.lower())
            weight = best_sim
            sim_sum += weight * valueSim
            weight_sum += weight
            m += 1

    avgSim = sim_sum / weight_sum if weight_sum > 0 else 0.0

    #hsm
    mw1 = set()
    mw2 = set()
    for k in nmki:
        mw1 |= DataCleaner.extract_title_model_words(
            DataCleaner.clean_value(str(p1.attributes.get(k, "")))
        )
    for k in nmkj:
        mw2 |= DataCleaner.extract_title_model_words(
            DataCleaner.clean_value(str(p2.attributes.get(k, "")))
        )
    if mw1 or mw2:
        inter = len(mw1 & mw2)
        uni = len(mw1 | mw2)
        mwPerc = inter / uni if uni > 0 else 0.0
    else:
        mwPerc = 0.0

    min_features = min(len(keys1), len(keys2)) if (keys1 and keys2) else 1
    return avgSim, mwPerc, m, min_features


def hsim_from_components(avgSim, mwPerc, titleSim, m, min_features, mu):
    if min_features <= 0:
        return 0.0
    if titleSim == -1:
        theta1 = m / min_features
        theta2 = 1.0 - theta1
        hSim = theta1 * avgSim + theta2 * mwPerc
    else:
        theta1 = (1.0 - mu) * (m / min_features)
        theta2 = 1.0 - mu - theta1
        hSim = theta1 * avgSim + theta2 * mwPerc + mu * titleSim
    # clamp
    return max(0.0, min(1.0, hSim))

#single linkage clustering
def hierarchical_clustering_from_pairs(pairs, struct_comps, title_sims, mu, eps):
    edges = []
    for (i, j) in pairs:
        key = (i, j) if (i, j) in struct_comps else (j, i)
        avgSim, mwPerc, m, min_features = struct_comps[key]
        tSim = title_sims.get(key, -1.0)
        hSim = hsim_from_components(avgSim, mwPerc, tSim, m, min_features, mu)
        dist = 1.0 - hSim
        edges.append((dist, i, j))

    edges.sort(key=lambda x: x[0])

    parent = {}
    clusters = {}

    def find(x):
        if parent[x] != x:
            parent[x] = find(parent[x])
        return parent[x]

    def union(x, y):
        rx = find(x)
        ry = find(y)
        if rx == ry:
            return
        if len(clusters[rx]) < len(clusters[ry]):
            rx, ry = ry, rx
        parent[ry] = rx
        clusters[rx].update(clusters[ry])
        del clusters[ry]

    nodes = set()
    for (i, j) in pairs:
        nodes.add(i)
        nodes.add(j)
    for v in nodes:
        parent[v] = v
        clusters[v] = {v}

    for dist, i, j in edges:
        if dist > eps:
            break
        union(i, j)

    #extract predicted duplicate pairs from clusters
    pred_pairs = set()
    for members in clusters.values():
        if len(members) > 1:
            mem_list = sorted(members)
            for a_idx in range(len(mem_list)):
                for b_idx in range(a_idx + 1, len(mem_list)):
                    pred_pairs.add((mem_list[a_idx], mem_list[b_idx]))
    return pred_pairs





def load_products():
    global TITLE_STOPWORDS

    products = []
    try:
        with open(DATASET_FILE, "r") as f:
            data = json.load(f)

        raw_items = []

        if isinstance(data, dict):
            for model_id, items in data.items():
                for item in items:
                    raw_items.append(
                        {
                            "shop": item.get("shop"),
                            "modelID": model_id,
                            "title": item.get("title", "") or "",
                            "featuresMap": item.get("featuresMap", {}) or {},
                        }
                    )
        elif isinstance(data, list):
            for obj in data:
                raw_items.append(
                    {
                        "shop": obj.get("shop"),
                        "modelID": obj.get("modelID"),
                        "title": obj.get("title", "") or "",
                        "featuresMap": obj.get("featuresMap", {}) or {},
                    }
                )
        else:
            print("Unsupported JSON structure.")
            return []

        # Compute DF-based title stopwords once over entire dataset
        TITLE_STOPWORDS = compute_title_stopwords(raw_items, threshold=0.30)
        print(f"Identified {len(TITLE_STOPWORDS)} high-frequency title tokens as stopwords.")

        idx = 0
        for item in raw_items:
            p = Product(
                shop=item["shop"],
                model_id=item["modelID"],
                title=item["title"],
                attributes=item["featuresMap"],
                original_idx=idx,
            )
            products.append(p)
            idx += 1

    except Exception as e:
        print("Error loading data:", e)

    return products


#evaluation
def run_evaluation():
    random.seed(42)
    np.random.seed(42)

    all_products = load_products()
    n_total = len(all_products)
    print(f"Loaded {n_total} products")
    if n_total < 2:
        return

    agg = defaultdict(list)

    for boot in range(NUM_BOOTSTRAPS):
        indices = [random.randint(0, n_total - 1) for _ in range(n_total)]
        in_bag = set(indices)
        out_of_bag = list(set(range(n_total)) - in_bag)

        if len(out_of_bag) < 2:
            continue

        test_instances = [Instance(i, all_products[idx]) for i, idx in enumerate(out_of_bag)]
        id2inst = {inst.id: inst for inst in test_instances}

        #use test to build vocab
        vocab = set()
        for inst in test_instances:
            vocab |= inst.model_words
        vocab_map = {w: i for i, w in enumerate(sorted(vocab))}
        vocab_size = len(vocab_map)
        if vocab_size == 0:
            continue

        n = max(1, int(0.5 * vocab_size))  # signature length = 50% vocab

        #golden standards
        true_pairs = set()
        possible_pairs = set()

        #model ids
        model_map = defaultdict(list)
        for inst in test_instances:
            model_map[inst.model_id].append(inst.id)

        for ids in model_map.values():
            if len(ids) > 1:
                for i_idx in range(len(ids)):
                    for j_idx in range(i_idx + 1, len(ids)):
                        i_id = ids[i_idx]
                        j_id = ids[j_idx]
                        pi = id2inst[i_id].product
                        pj = id2inst[j_id].product
                        #same shop / different brand filter (golden standard)
                        if pi.shop == pj.shop:
                            continue
                        if pi.brand and pj.brand and pi.brand != pj.brand:
                            continue
                        true_pairs.add(tuple(sorted((i_id, j_id))))

        #total possible comparisons
        m_test = len(test_instances)
        for i in range(m_test):
            for j in range(i + 1, m_test):
                pi = test_instances[i].product
                pj = test_instances[j].product
                if pi.shop == pj.shop:
                    continue
                if pi.brand and pj.brand and pi.brand != pj.brand:
                    continue
                possible_pairs.add((i, j))

        if not true_pairs or not possible_pairs:
            continue

        max_pairs = len(possible_pairs)


        lsh = LSH(n, vocab_size)
        lsh.compute_signatures(test_instances, vocab_map)

        print(
            f"Bootstrap {boot + 1}/{NUM_BOOTSTRAPS}: "
            f"|test|={len(test_instances)}, |true_pairs|={len(true_pairs)}, "
            f"|universe|={max_pairs}"
        )

        for t in T_VALUES:
            b, r_eff, t_est = choose_b_r(n, t)
            if b < 1:
                continue

            cand_pairs = lsh.get_candidates(test_instances, b)
            num_cand = len(cand_pairs)
            frac_comp = num_cand / max_pairs if max_pairs > 0 else 0.0

            #lsh metrics
            cand_true = set()
            for (i, j) in cand_pairs:
                pair = tuple(sorted((i, j)))
                if pair in true_pairs:
                    cand_true.add(pair)
            tp_lsh = len(cand_true)

            #PQ- average duplicates found per comparison
            pq = (2.0 * tp_lsh / num_cand) if num_cand > 0 else 0.0
            #PC- fraction of all true duplicates found
            pc = tp_lsh / len(true_pairs) if true_pairs else 0.0
            #F1* (harmonic mean of PC and PQ)
            f1_star = (2 * pq * pc / (pq + pc)) if (pq + pc) > 0 else 0.0

            #MSM grid search
            if not cand_pairs:
                best_f1 = 0.0
            else:
                gamma = GAMMA_GRID[0]

                struct_comps = {}
                for (i, j) in cand_pairs:
                    key = (i, j) if i <= j else (j, i)
                    if key in struct_comps:
                        continue
                    p1 = id2inst[i].product
                    p2 = id2inst[j].product
                    avgSim, mwPerc, m, min_features = compute_structural_components(
                        p1, p2, gamma
                    )
                    struct_comps[key] = (avgSim, mwPerc, m, min_features)


                title_sim_cache = defaultdict(dict)
                beta = BETA_GRID[0]
                for (i, j) in cand_pairs:
                    key = (i, j) if i <= j else (j, i)
                    p1 = id2inst[i].product
                    p2 = id2inst[j].product
                    for alpha in ALPHA_GRID:
                        if alpha in title_sim_cache[key]:
                            continue
                        title_sim_cache[key][alpha] = tmwm_title_sim(p1, p2, alpha, beta)

                best_f1 = 0.0

                for alpha in ALPHA_GRID:

                    title_sims_alpha = {k: v[alpha] for k, v in title_sim_cache.items()}
                    for mu in MU_GRID:
                        for eps in EPS_GRID:
                            pred_pairs = hierarchical_clustering_from_pairs(
                                cand_pairs, struct_comps, title_sims_alpha, mu, eps
                            )

                            tp = len(pred_pairs & true_pairs)
                            fp = len(pred_pairs) - tp
                            fn = len(true_pairs) - tp

                            if tp + fp == 0:
                                precision = 0.0
                            else:
                                precision = tp / (tp + fp)
                            if tp + fn == 0:
                                recall = 0.0
                            else:
                                recall = tp / (tp + fn)

                            if precision + recall == 0:
                                f1 = 0.0
                            else:
                                f1 = 2 * precision * recall / (precision + recall)

                            if f1 > best_f1:
                                best_f1 = f1

            agg[t].append(
                {
                    "b": b,
                    "r_eff": r_eff,
                    "t_est": t_est,
                    "frac_comp": frac_comp,
                    "PQ": pq,
                    "PC": pc,
                    "F1_star": f1_star,
                    "F1": best_f1,
                }
            )

    #table with all alues
    print(f"\n AVERAGED RESULTS OVER {NUM_BOOTSTRAPS} BOOTSTRAPS")
    header = "{:>4}\t{:>4}\t{:>5}\t{:>5}\t{:>9}\t{:>6}\t{:>6}\t{:>6}\t{:>6}".format(
        "t", "b", "r_eff", "t_est", "frac_comp", "PQ", "PC", "F1*", "F1"
    )
    print(header)
    for t in sorted(agg.keys()):
        stats = agg[t]
        avg_b = np.mean([s["b"] for s in stats])
        avg_r = np.mean([s["r_eff"] for s in stats])
        avg_t_est = np.mean([s["t_est"] for s in stats])
        avg_frac = np.mean([s["frac_comp"] for s in stats])
        avg_pq = np.mean([s["PQ"] for s in stats])
        avg_pc = np.mean([s["PC"] for s in stats])
        avg_f1s = np.mean([s["F1_star"] for s in stats])
        avg_f1 = np.mean([s["F1"] for s in stats])

        print(
            "{:>4.2f}\t{:>4.0f}\t{:>5.2f}\t{:>5.3f}\t{:>9.4f}\t{:>6.4f}\t{:>6.4f}\t{:>6.4f}\t{:>6.4f}".format(
                t, avg_b, avg_r, avg_t_est, avg_frac, avg_pq, avg_pc, avg_f1s, avg_f1
            )
        )
        
if __name__ == "__main__":
    run_evaluation()

